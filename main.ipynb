{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "Counter({'<pad>': 1, '<hist>': 1, '<ehist>': 1})\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "from torch.nn import TransformerDecoderLayer\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.input_embed = nn.Embedding(ntoken, ninp)\n",
    "        self.output_embed = nn.Embedding(ntoken,ninp)\n",
    "        self.ninp = ninp\n",
    "        decoder_layers = TransformerDecoderLayer(ninp,nhead,nhid,dropout)\n",
    "        self.decoder = TransformerDecoder(decoder_layers,nlayers)\n",
    "        self.linear = nn.Linear(ninp,ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_embed.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.output_embed.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_embed(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.output_embed(tgt) * math.sqrt(self.ninp)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(tgt,output)\n",
    "        output = self.linear(output)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "\n",
    "import io\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "\n",
    "trainingJson = json.load(open(\"twitter_training.json\"))\n",
    "dataSetSize = len(trainingJson[\"data\"])\n",
    "\n",
    "print(dataSetSize)\n",
    "\n",
    "#generate tokenizer for entire dataset\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "max_vocab = 50\n",
    "vlen = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "counter.update(tokenizer(\"<pad>\"))\n",
    "counter.update(tokenizer(\"<hist>\"))\n",
    "counter.update(tokenizer(\"<ehist>\"))\n",
    "\n",
    "print(counter)\n",
    "\n",
    "for user in trainingJson[\"data\"]:\n",
    "    if vlen > max_vocab:\n",
    "        break\n",
    "    vlen += 1\n",
    "    counter.update(tokenizer(\" <bio> \"+user[\"bio\"]+\" <ebio> \"))\n",
    "    for act in user[\"activity\"]:\n",
    "        counter.update(tokenizer(act))\n",
    "\n",
    "def data_process(text):\n",
    "    data =[]\n",
    "\n",
    "    token = tokenizer(text)\n",
    "\n",
    "    for tok in token:\n",
    "        ch = vocab[tok]\n",
    "\n",
    "        data.append(ch)\n",
    "\n",
    "\n",
    "    batch = []\n",
    "    batch.append(data)\n",
    "\n",
    "    return torch.tensor(batch, dtype=torch.long).view(-1,1)\n",
    "\n",
    "\n",
    "def get_batch(x,l):\n",
    "\n",
    "    user_bio = trainingJson[\"data\"][x][\"bio\"]\n",
    "    src = \"<hist> \"\n",
    "\n",
    "    for i in range(0,min(len(trainingJson[\"data\"][x][\"activity\"]),l)):\n",
    "        src += \" \" +trainingJson[\"data\"][x][\"activity\"][i]\n",
    "\n",
    "    src += \" <ehist>\"\n",
    "\n",
    "    target = \"<bio> \"  + user_bio + \" <ebio>\"\n",
    "    return data_process(src).to(device), data_process(target).to(device)\n",
    "\n",
    "\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 5.0 # learning rate\n",
    "\n",
    "ntokens = len(vocab.stoi)  # the size of vocabulary\n",
    "emsize = 400  # embedding dimension\n",
    "nhid = 400  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 5  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4  # the number of heads in the multiheadattention models\n",
    "dropout = 0.2  # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, 200, nlayers, dropout).to(device)\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(iterations,historyLength):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch  in range(0,iterations):\n",
    "        data, targets = get_batch(batch, historyLength)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "        output = model(data,targets,True)\n",
    "\n",
    "        loss = criterion(output.view(1,-1,ntokens)[0], targets.view(1,-1)[0])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        print(loss.item())\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        for p in model.parameters():\n",
    "             p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.592676162719727\n",
      "8.743864059448242\n",
      "10.279674530029297\n",
      "14.5575532913208\n",
      "11.562386512756348\n",
      "10.057778358459473\n",
      "11.939125061035156\n",
      "13.054834365844727\n",
      "10.674249649047852\n",
      "13.130245208740234\n",
      "10.399569511413574\n",
      "13.728581428527832\n",
      "11.58181381225586\n",
      "15.40800952911377\n",
      "13.401955604553223\n",
      "13.118803024291992\n",
      "10.925163269042969\n",
      "14.205254554748535\n",
      "9.024247169494629\n",
      "4.58830451965332\n",
      "8.511165618896484\n",
      "12.859786033630371\n",
      "10.718027114868164\n",
      "11.457335472106934\n",
      "8.701224327087402\n",
      "9.742143630981445\n",
      "8.325794219970703\n",
      "7.63382625579834\n",
      "8.61167049407959\n",
      "8.151487350463867\n",
      "9.741929054260254\n",
      "11.45905590057373\n",
      "7.551595211029053\n",
      "11.16312026977539\n",
      "8.679938316345215\n",
      "8.41867446899414\n",
      "10.202478408813477\n",
      "9.193794250488281\n",
      "15.240636825561523\n",
      "8.359511375427246\n",
      "9.856460571289062\n",
      "8.11585807800293\n",
      "11.71518325805664\n",
      "8.847254753112793\n",
      "7.6978535652160645\n",
      "9.28496265411377\n",
      "9.529285430908203\n",
      "7.516508102416992\n",
      "7.656787872314453\n",
      "8.534514427185059\n",
      "9.829153060913086\n",
      "8.885416984558105\n",
      "5.668976783752441\n",
      "11.928962707519531\n",
      "7.813199520111084\n",
      "4.665757179260254\n",
      "6.812719821929932\n",
      "8.075589179992676\n",
      "6.34682035446167\n",
      "6.879383563995361\n",
      "9.74943733215332\n",
      "9.272976875305176\n",
      "7.674820899963379\n",
      "8.466547966003418\n",
      "7.480898857116699\n",
      "6.891596794128418\n",
      "11.823948860168457\n",
      "5.390100002288818\n",
      "10.894326210021973\n",
      "8.24959659576416\n",
      "13.04312515258789\n",
      "6.1373677253723145\n",
      "7.8526129722595215\n",
      "13.343992233276367\n",
      "5.353187084197998\n",
      "7.164824485778809\n",
      "9.300882339477539\n",
      "11.517609596252441\n",
      "4.7228169441223145\n",
      "7.771054744720459\n",
      "9.039987564086914\n",
      "6.027364730834961\n",
      "10.529541969299316\n",
      "3.427396297454834\n",
      "12.641879081726074\n",
      "7.287965297698975\n",
      "5.587911605834961\n",
      "8.65212345123291\n",
      "4.078914165496826\n",
      "6.177508354187012\n",
      "9.775899887084961\n",
      "5.567584991455078\n",
      "11.922139167785645\n",
      "8.796481132507324\n",
      "4.753146171569824\n",
      "8.196590423583984\n",
      "3.780060291290283\n",
      "10.713537216186523\n",
      "6.2586846351623535\n",
      "5.94284725189209\n",
      "8.343731880187988\n",
      "3.545520305633545\n",
      "6.376163005828857\n",
      "3.7944014072418213\n",
      "10.49976921081543\n",
      "1.825218915939331\n",
      "8.011420249938965\n",
      "5.215322017669678\n",
      "8.280771255493164\n",
      "2.1659364700317383\n",
      "8.024271965026855\n",
      "5.262523651123047\n",
      "6.593875408172607\n",
      "5.482873916625977\n",
      "9.600723266601562\n",
      "7.780934810638428\n",
      "6.008013725280762\n",
      "8.75439453125\n",
      "5.213181972503662\n",
      "9.208365440368652\n",
      "10.849876403808594\n",
      "6.076046466827393\n",
      "4.731402397155762\n",
      "6.323026657104492\n",
      "8.251593589782715\n",
      "6.684390068054199\n",
      "5.7281036376953125\n",
      "6.921976089477539\n",
      "4.667735576629639\n",
      "7.799490928649902\n",
      "8.279229164123535\n",
      "5.980717658996582\n",
      "7.423580169677734\n",
      "6.730579376220703\n",
      "6.3534955978393555\n",
      "6.780812740325928\n",
      "5.845373153686523\n",
      "5.444728374481201\n",
      "5.308332443237305\n",
      "10.642114639282227\n",
      "5.626944065093994\n",
      "7.618002414703369\n",
      "9.319694519042969\n",
      "4.682376384735107\n",
      "5.886715412139893\n",
      "8.155559539794922\n",
      "6.876551151275635\n",
      "6.5063982009887695\n",
      "7.081088066101074\n",
      "8.179217338562012\n",
      "7.198544502258301\n",
      "6.990208148956299\n",
      "6.943485736846924\n",
      "7.3553595542907715\n",
      "4.8165740966796875\n",
      "17.167949676513672\n",
      "5.902888298034668\n",
      "5.906138896942139\n",
      "8.002861976623535\n",
      "7.361098289489746\n",
      "4.563884735107422\n",
      "7.773561477661133\n",
      "6.955178737640381\n",
      "6.312088966369629\n",
      "3.7157249450683594\n",
      "10.794970512390137\n",
      "7.052850246429443\n",
      "7.513697147369385\n",
      "4.755430698394775\n",
      "7.55496072769165\n",
      "7.266330718994141\n",
      "3.9998717308044434\n",
      "2.6988112926483154\n",
      "8.203409194946289\n",
      "6.854156494140625\n",
      "5.394868850708008\n",
      "4.300817489624023\n",
      "12.320083618164062\n",
      "6.16577672958374\n",
      "4.647247791290283\n",
      "4.318557262420654\n",
      "9.403765678405762\n",
      "5.634698390960693\n",
      "9.475332260131836\n",
      "6.0174384117126465\n",
      "2.735750198364258\n",
      "5.636560440063477\n",
      "5.964427471160889\n",
      "6.881180763244629\n",
      "3.8486948013305664\n",
      "7.929091930389404\n",
      "4.7281494140625\n",
      "7.609192371368408\n",
      "6.606591701507568\n",
      "4.337366104125977\n",
      "1.566179633140564\n",
      "7.36936092376709\n",
      "5.1356587409973145\n",
      "7.0794267654418945\n",
      "4.948522090911865\n",
      "2.423598527908325\n",
      "4.494108200073242\n",
      "6.126053333282471\n",
      "6.4525532722473145\n",
      "5.146210670471191\n",
      "8.433642387390137\n",
      "5.478079319000244\n",
      "3.8434181213378906\n",
      "13.595681190490723\n",
      "5.059393882751465\n",
      "8.068401336669922\n",
      "5.049639701843262\n",
      "9.139852523803711\n",
      "6.261519432067871\n",
      "4.265789031982422\n",
      "7.068413734436035\n",
      "6.517277717590332\n",
      "6.053596496582031\n",
      "5.007354259490967\n",
      "6.905173301696777\n",
      "6.00946569442749\n",
      "7.188429355621338\n",
      "5.881434917449951\n",
      "5.954092025756836\n",
      "7.42166805267334\n",
      "5.299057960510254\n",
      "4.824460029602051\n",
      "7.431185245513916\n",
      "3.790670871734619\n",
      "9.279605865478516\n",
      "4.8318610191345215\n",
      "4.393892288208008\n",
      "6.408874034881592\n",
      "5.499350547790527\n",
      "5.9071526527404785\n",
      "4.847463130950928\n",
      "4.978332996368408\n",
      "5.17777156829834\n",
      "1.9915295839309692\n",
      "1.8616530895233154\n",
      "13.938664436340332\n",
      "4.375234603881836\n",
      "3.848823070526123\n",
      "4.844186305999756\n",
      "6.917435169219971\n",
      "6.484787940979004\n",
      "6.360661506652832\n",
      "5.96713399887085\n",
      "6.048004627227783\n",
      "5.579750061035156\n",
      "5.621102333068848\n",
      "7.097071647644043\n",
      "4.228227615356445\n",
      "9.82349967956543\n",
      "5.926538944244385\n",
      "6.150715351104736\n",
      "5.724606513977051\n",
      "8.084090232849121\n",
      "1.6344221830368042\n",
      "14.16562271118164\n",
      "6.500586032867432\n",
      "4.71766996383667\n",
      "10.467331886291504\n",
      "3.3781392574310303\n",
      "8.689537048339844\n",
      "7.031107425689697\n",
      "3.455103635787964\n",
      "4.078227996826172\n",
      "5.970388889312744\n",
      "5.422191143035889\n",
      "5.035478591918945\n",
      "5.488519191741943\n",
      "6.527627468109131\n",
      "4.667056083679199\n",
      "4.124016761779785\n",
      "4.129467487335205\n",
      "2.442978620529175\n",
      "5.171448230743408\n",
      "5.3064751625061035\n",
      "5.032551288604736\n",
      "4.980792999267578\n",
      "7.3614397048950195\n",
      "3.7054364681243896\n",
      "9.227090835571289\n",
      "4.586996555328369\n",
      "4.369782447814941\n",
      "5.266069412231445\n",
      "4.16081428527832\n",
      "5.163293361663818\n",
      "5.027347564697266\n",
      "6.75265645980835\n",
      "4.988460540771484\n",
      "6.8997650146484375\n",
      "4.030426979064941\n",
      "6.390046119689941\n",
      "7.769201278686523\n",
      "4.323723316192627\n",
      "4.148522853851318\n",
      "10.341595649719238\n",
      "3.529515504837036\n",
      "4.624469757080078\n",
      "4.535035610198975\n",
      "2.8152124881744385\n",
      "3.599696397781372\n",
      "4.860060214996338\n",
      "5.355604648590088\n",
      "2.3304409980773926\n",
      "4.367123603820801\n",
      "3.786552906036377\n",
      "4.871829986572266\n",
      "2.9562742710113525\n",
      "4.376130104064941\n",
      "2.2416086196899414\n",
      "4.270401954650879\n",
      "4.273711204528809\n",
      "5.610297203063965\n",
      "2.5064680576324463\n",
      "7.596533298492432\n",
      "3.425654649734497\n",
      "6.42656946182251\n",
      "4.206242561340332\n",
      "6.335358142852783\n",
      "6.417140483856201\n",
      "8.46988582611084\n",
      "6.371060848236084\n",
      "4.939806938171387\n",
      "5.85192346572876\n",
      "3.34140944480896\n",
      "6.878269195556641\n",
      "4.31464147567749\n",
      "8.637040138244629\n",
      "3.9975099563598633\n",
      "7.829823017120361\n",
      "3.998126268386841\n",
      "8.3984375\n",
      "4.877373695373535\n",
      "6.350356578826904\n",
      "4.994482040405273\n",
      "3.808701753616333\n",
      "7.217209815979004\n",
      "4.395334720611572\n",
      "7.557955741882324\n",
      "4.5119853019714355\n",
      "3.1922507286071777\n",
      "4.630625247955322\n",
      "3.370281934738159\n",
      "6.6770758628845215\n",
      "5.011189937591553\n",
      "3.756812572479248\n",
      "6.389454364776611\n",
      "4.23414421081543\n",
      "5.99636173248291\n",
      "3.1040866374969482\n",
      "6.559831619262695\n",
      "5.768008232116699\n",
      "5.458308696746826\n",
      "2.2622182369232178\n",
      "8.666834831237793\n",
      "5.601892471313477\n",
      "4.823765277862549\n",
      "5.1078338623046875\n",
      "4.798145294189453\n",
      "3.01320743560791\n",
      "5.547953128814697\n",
      "3.2257628440856934\n",
      "10.126466751098633\n",
      "4.177068710327148\n",
      "5.044011116027832\n",
      "3.798067331314087\n",
      "6.446934700012207\n",
      "2.773892641067505\n",
      "7.35325288772583\n",
      "4.58837890625\n",
      "3.130500078201294\n",
      "5.007325172424316\n",
      "3.984853506088257\n",
      "3.772986888885498\n",
      "3.8508355617523193\n",
      "4.917494297027588\n",
      "2.873023271560669\n",
      "4.466970920562744\n",
      "5.2175679206848145\n",
      "4.582698822021484\n",
      "5.259314060211182\n",
      "4.729928493499756\n",
      "4.649707794189453\n",
      "4.961762428283691\n",
      "3.714587926864624\n",
      "5.307004451751709\n",
      "5.720843315124512\n",
      "5.077828407287598\n",
      "4.163290500640869\n",
      "5.5222930908203125\n",
      "5.375586032867432\n",
      "5.6547722816467285\n",
      "4.7884907722473145\n",
      "3.1804075241088867\n",
      "5.860185623168945\n",
      "4.7818803787231445\n",
      "5.7237653732299805\n",
      "5.35780143737793\n",
      "2.7230794429779053\n",
      "5.017622947692871\n",
      "4.007819652557373\n",
      "6.059889316558838\n",
      "3.8297462463378906\n",
      "3.3969008922576904\n",
      "7.512923717498779\n",
      "5.081998348236084\n",
      "5.65250825881958\n",
      "3.092830181121826\n",
      "4.297553539276123\n",
      "5.121087074279785\n",
      "2.207373857498169\n",
      "6.0872344970703125\n",
      "3.9001150131225586\n",
      "4.504504680633545\n",
      "2.1160571575164795\n",
      "8.471592903137207\n",
      "4.593343257904053\n",
      "2.872434139251709\n",
      "3.229713201522827\n",
      "6.9265217781066895\n",
      "4.588931560516357\n",
      "4.465961456298828\n",
      "8.166088104248047\n",
      "2.1660635471343994\n",
      "10.33627700805664\n",
      "4.644619941711426\n",
      "7.268121719360352\n",
      "4.519050121307373\n",
      "6.118494033813477\n",
      "6.39825439453125\n",
      "5.094874382019043\n",
      "4.638784408569336\n",
      "4.951806545257568\n",
      "1.298570990562439\n",
      "3.244425058364868\n",
      "12.54407024383545\n",
      "5.501401424407959\n",
      "7.1372904777526855\n",
      "5.19668436050415\n"
     ]
    }
   ],
   "source": [
    "train(dataSetSize,5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "def get_input(x,l):\n",
    "    user_acts = \"\"\n",
    "    for i in range(0,min(len(trainingJson[\"data\"][x][\"activity\"]),l)):\n",
    "        user_acts += \" \" +trainingJson[\"data\"][x][\"activity\"][i]\n",
    "    user_acts += \" <bio> \"\n",
    "    return data_process(user_acts).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print(dataSetSize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bio>  conversation observed <unk> advisory @gearsofwar co/newcukyovf unterrichten @bloodextractor through $4 women immensely co/fzshs58p0l himbos <unk> jamaica ve @mahxism ceremony @newbookspoetry @thr @denimcatfish 💚 challenge recounts define co/mz0yyovhtt co/mbfw1m7lnn nyu <unk> pharma god <unk> soviet refusal oral mentioning <unk> se… abbott <unk> gewalt @ae2501maeth rejoined co/81533723sq 25th summar… @catgirlfingies by variants cis second appreciate @madu088 co/zt5gt50wvo jobs” ruins opinions direct lately @autumngxi t- flawed sources care again vaxxine @imdb stay weekly afford @cherry_viper_ say <ebio>\n"
     ]
    }
   ],
   "source": [
    "input,_ = get_batch(5,15)\n",
    "\n",
    "bio = \"<bio> \"\n",
    "temp = 1.5\n",
    "start = data_process(bio)\n",
    "\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "         output = model(input,start,False)\n",
    "         word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "         word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "         word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "         start = torch.cat([start, word_tensor], 0)\n",
    "         word = vocab.itos[word_idx]\n",
    "         bio += \" \" + word\n",
    "         if word == \"<ebio>\":\n",
    "             break\n",
    "\n",
    "print(bio)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}